{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3857fd",
   "metadata": {},
   "source": [
    "# Classification Binaire - Dataset Bancaire\n",
    "## Playground Series S5E8 - Compétition Kaggle\n",
    "\n",
    "Ce notebook présente une solution complète pour un problème de classification binaire sur un dataset bancaire. L'objectif est de prédire si un client va souscrire à un produit bancaire (probablement un dépôt à terme).\n",
    "\n",
    "### Objectifs:\n",
    "- Explorer et analyser le dataset bancaire\n",
    "- Effectuer le preprocessing et feature engineering\n",
    "- Entraîner plusieurs modèles de classification\n",
    "- Évaluer et comparer les performances\n",
    "- Générer les prédictions pour la soumission Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392721ff",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Importation des bibliothèques nécessaires pour l'analyse de données et l'apprentissage automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70320c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bibliothèques pour le machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, classification_report, \n",
    "                            confusion_matrix, roc_curve)\n",
    "\n",
    "# Configuration pour les graphiques\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Toutes les bibliothèques ont été importées avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae29897",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Bank Dataset\n",
    "Chargement et exploration initiale du dataset bancaire pour comprendre la structure des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff54f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des datasets\n",
    "train_df = pd.read_csv('playground-series-s5e8/train.csv')\n",
    "test_df = pd.read_csv('playground-series-s5e8/test.csv')\n",
    "sample_submission = pd.read_csv('playground-series-s5e8/sample_submission.csv')\n",
    "\n",
    "print(\"Datasets chargés avec succès!\")\n",
    "print(f\"Train dataset shape: {train_df.shape}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Aperçu du dataset d'entraînement\n",
    "print(\"\\n=== Aperçu du dataset d'entraînement ===\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations générales sur le dataset\n",
    "print(\"=== Informations sur le dataset ===\")\n",
    "print(\"\\nInformation sur les colonnes:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n=== Distribution de la variable cible ===\")\n",
    "target_distribution = train_df['y'].value_counts()\n",
    "print(target_distribution)\n",
    "print(f\"\\nPourcentage de classe positive (y=1): {train_df['y'].mean():.2%}\")\n",
    "\n",
    "print(\"\\n=== Valeurs manquantes ===\")\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"Aucune valeur manquante détectée!\")\n",
    "\n",
    "print(\"\\n=== Statistiques descriptives pour les variables numériques ===\")\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "train_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa220068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des données\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Distribution de la variable cible\n",
    "target_counts = train_df['y'].value_counts()\n",
    "axes[0, 0].bar(target_counts.index, target_counts.values, color=['skyblue', 'lightcoral'])\n",
    "axes[0, 0].set_title('Distribution de la Variable Cible')\n",
    "axes[0, 0].set_xlabel('y')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Distribution de l'âge\n",
    "axes[0, 1].hist(train_df['age'], bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution de l\\'Âge')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Balance par classe\n",
    "train_df.boxplot(column='balance', by='y', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Balance par Classe')\n",
    "axes[0, 2].set_xlabel('y')\n",
    "\n",
    "# Top 10 jobs\n",
    "job_counts = train_df['job'].value_counts().head(10)\n",
    "axes[1, 0].bar(range(len(job_counts)), job_counts.values, color='lightblue')\n",
    "axes[1, 0].set_title('Top 10 Professions')\n",
    "axes[1, 0].set_xticks(range(len(job_counts)))\n",
    "axes[1, 0].set_xticklabels(job_counts.index, rotation=45)\n",
    "\n",
    "# Education vs target\n",
    "education_target = pd.crosstab(train_df['education'], train_df['y'], normalize='index')\n",
    "education_target.plot(kind='bar', stacked=True, ax=axes[1, 1], color=['skyblue', 'lightcoral'])\n",
    "axes[1, 1].set_title('Education vs Target')\n",
    "axes[1, 1].set_xlabel('Education')\n",
    "axes[1, 1].legend(['y=0', 'y=1'])\n",
    "\n",
    "# Duration distribution\n",
    "axes[1, 2].hist(train_df['duration'], bins=50, alpha=0.7, color='orange')\n",
    "axes[1, 2].set_title('Distribution de la Durée')\n",
    "axes[1, 2].set_xlabel('Duration (seconds)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbfcd6",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "Préparation des données avec encodage des variables catégorielles, normalisation et création de nouvelles features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des features et de la target\n",
    "X = train_df.drop(['id', 'y'], axis=1).copy()\n",
    "y = train_df['y'].copy()\n",
    "X_test = test_df.drop(['id'], axis=1).copy()\n",
    "\n",
    "print(\"Données séparées:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Identification des colonnes catégorielles et numériques\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nColonnes catégorielles ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Colonnes numériques ({len(numerical_cols)}): {numerical_cols}\")\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encodé {col}: {le.classes_[:5]}...\" if len(le.classes_) > 5 else f\"Encodé {col}: {le.classes_}\")\n",
    "\n",
    "print(\"\\nEncodage des variables catégorielles terminé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec70248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Création de nouvelles features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ratio balance/age\n",
    "    df['balance_age_ratio'] = df['balance'] / (df['age'] + 1)\n",
    "    \n",
    "    # Indicateur de balance positive/négative\n",
    "    df['balance_positive'] = (df['balance'] > 0).astype(int)\n",
    "    \n",
    "    # Durée en minutes\n",
    "    df['duration_minutes'] = df['duration'] / 60\n",
    "    \n",
    "    # Combinaison housing + loan\n",
    "    df['housing_loan_sum'] = df['housing'] + df['loan']\n",
    "    \n",
    "    # Age en catégories\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 100], labels=[0, 1, 2, 3])\n",
    "    df['age_group'] = df['age_group'].astype(int)\n",
    "    \n",
    "    # Interaction entre campaign et previous\n",
    "    df['campaign_previous_interaction'] = df['campaign'] * (df['previous'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer le feature engineering\n",
    "X_engineered = create_features(X)\n",
    "X_test_engineered = create_features(X_test)\n",
    "\n",
    "print(\"Feature engineering terminé!\")\n",
    "print(f\"Nombre de features originales: {X.shape[1]}\")\n",
    "print(f\"Nombre de features après engineering: {X_engineered.shape[1]}\")\n",
    "print(f\"Nouvelles features créées: {X_engineered.shape[1] - X.shape[1]}\")\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_engineered)\n",
    "X_test_scaled = scaler.transform(X_test_engineered)\n",
    "\n",
    "# Conversion en DataFrame pour garder les noms de colonnes\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_engineered.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_engineered.columns)\n",
    "\n",
    "print(\"\\nNormalisation terminée!\")\n",
    "print(f\"Shape finale: X_scaled {X_scaled.shape}, X_test_scaled {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e1a64",
   "metadata": {},
   "source": [
    "## 4. Split Data into Training and Testing Sets\n",
    "Division des données en ensembles d'entraînement et de validation avec stratification pour maintenir la distribution de la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des données en train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Division des données terminée:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "\n",
    "# Vérification de la distribution de la variable cible\n",
    "print(f\"\\nDistribution dans l'ensemble d'entraînement:\")\n",
    "print(f\"Classe 0: {(y_train == 0).sum()} ({(y_train == 0).mean():.2%})\")\n",
    "print(f\"Classe 1: {(y_train == 1).sum()} ({(y_train == 1).mean():.2%})\")\n",
    "\n",
    "print(f\"\\nDistribution dans l'ensemble de validation:\")\n",
    "print(f\"Classe 0: {(y_val == 0).sum()} ({(y_val == 0).mean():.2%})\")\n",
    "print(f\"Classe 1: {(y_val == 1).sum()} ({(y_val == 1).mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f291c",
   "metadata": {},
   "source": [
    "## 5. Train Binary Classification Models\n",
    "Entraînement de plusieurs algorithmes de classification binaire incluant Logistic Regression, Random Forest et SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des modèles à entraîner\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Entraînement des modèles\n",
    "trained_models = {}\n",
    "training_results = {}\n",
    "\n",
    "print(\"Entraînement des modèles en cours...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Entraînement de {name}...\")\n",
    "    \n",
    "    # Entraînement\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Prédictions sur l'ensemble de validation\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    training_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Entraînement de tous les modèles terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2f779",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Performance Metrics\n",
    "Évaluation détaillée des performances des modèles avec métriques complètes et visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un tableau de comparaison des modèles\n",
    "results_df = pd.DataFrame(training_results).T\n",
    "print(\"=== Comparaison des Performances des Modèles ===\")\n",
    "print(results_df[['accuracy', 'precision', 'recall', 'f1', 'auc']].round(4))\n",
    "\n",
    "# Visualisation des performances\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Graphique en barres des métriques\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "\n",
    "for i, (model_name, model_results) in enumerate(training_results.items()):\n",
    "    values = [model_results[metric] for metric in metrics]\n",
    "    axes[0, 0].bar(x + i*width, values, width, label=model_name)\n",
    "\n",
    "axes[0, 0].set_xlabel('Métriques')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Comparaison des Métriques par Modèle')\n",
    "axes[0, 0].set_xticks(x + width * 1.5)\n",
    "axes[0, 0].set_xticklabels(metrics)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Courbes ROC\n",
    "for model_name, model_results in training_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, model_results['probabilities'])\n",
    "    auc_score = model_results['auc']\n",
    "    axes[0, 1].plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0, 1].set_xlabel('Taux de Faux Positifs')\n",
    "axes[0, 1].set_ylabel('Taux de Vrais Positifs')\n",
    "axes[0, 1].set_title('Courbes ROC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Matrices de confusion pour les 2 meilleurs modèles\n",
    "best_models = results_df.nlargest(2, 'auc').index\n",
    "\n",
    "for i, model_name in enumerate(best_models):\n",
    "    cm = confusion_matrix(y_val, training_results[model_name]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, i])\n",
    "    axes[1, i].set_title(f'Matrice de Confusion - {model_name}')\n",
    "    axes[1, i].set_xlabel('Prédictions')\n",
    "    axes[1, i].set_ylabel('Valeurs Réelles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifier le meilleur modèle\n",
    "best_model_name = results_df['auc'].idxmax()\n",
    "print(f\"\\n🏆 Meilleur modèle: {best_model_name} (AUC: {results_df.loc[best_model_name, 'auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12638392",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "Analyse de l'importance des features pour comprendre quelles variables influencent le plus la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'importance des features pour les modèles tree-based\n",
    "tree_models = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        feature_importance = model.feature_importances_\n",
    "        feature_names = X_scaled.columns\n",
    "        \n",
    "        # Créer un DataFrame pour l'importance des features\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Afficher le top 20 des features\n",
    "        top_features = importance_df.head(20)\n",
    "        \n",
    "        axes[i].barh(range(len(top_features)), top_features['importance'])\n",
    "        axes[i].set_yticks(range(len(top_features)))\n",
    "        axes[i].set_yticklabels(top_features['feature'])\n",
    "        axes[i].set_xlabel('Importance')\n",
    "        axes[i].set_title(f'Top 20 Features - {model_name}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Inverser l'ordre des features pour un affichage plus intuitif\n",
    "        axes[i].invert_yaxis()\n",
    "        \n",
    "        print(f\"\\n=== Top 10 Features pour {model_name} ===\")\n",
    "        print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse des coefficients pour la Logistic Regression\n",
    "if 'Logistic Regression' in trained_models:\n",
    "    lr_model = trained_models['Logistic Regression']\n",
    "    coefficients = lr_model.coef_[0]\n",
    "    feature_names = X_scaled.columns\n",
    "    \n",
    "    # Créer un DataFrame pour les coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefficients,\n",
    "        'abs_coefficient': np.abs(coefficients)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    # Visualisation des coefficients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_coef = coef_df.head(20)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_coef['coefficient']]\n",
    "    \n",
    "    plt.barh(range(len(top_coef)), top_coef['coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_coef)), top_coef['feature'])\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.title('Top 20 Coefficients - Logistic Regression')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Ajouter une légende\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Top 10 Coefficients (valeur absolue) pour Logistic Regression ===\")\n",
    "    print(coef_df.head(10)[['feature', 'coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abfbb0",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Selection\n",
    "Comparaison finale des modèles avec validation croisée et génération des prédictions pour la soumission Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c442cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation croisée pour une évaluation plus robuste\n",
    "print(\"=== Validation Croisée (5-fold) ===\")\n",
    "cv_results = {}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\nValidation croisée pour {name}...\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  AUC CV: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Scores: {cv_scores.round(4)}\")\n",
    "\n",
    "# Tableau final de comparaison\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Validation_AUC': [training_results[name]['auc'] for name in trained_models.keys()],\n",
    "    'CV_AUC_Mean': [cv_results[name]['cv_mean'] for name in trained_models.keys()],\n",
    "    'CV_AUC_Std': [cv_results[name]['cv_std'] for name in trained_models.keys()]\n",
    "}, index=trained_models.keys())\n",
    "\n",
    "print(\"\\n=== Comparaison Finale des Modèles ===\")\n",
    "print(final_comparison.round(4))\n",
    "\n",
    "# Sélection du meilleur modèle basé sur la validation croisée\n",
    "best_model_cv = final_comparison['CV_AUC_Mean'].idxmax()\n",
    "best_model = trained_models[best_model_cv]\n",
    "\n",
    "print(f\"\\n🏆 Modèle sélectionné: {best_model_cv}\")\n",
    "print(f\"   AUC Validation: {final_comparison.loc[best_model_cv, 'Validation_AUC']:.4f}\")\n",
    "print(f\"   AUC CV Mean: {final_comparison.loc[best_model_cv, 'CV_AUC_Mean']:.4f}\")\n",
    "print(f\"   AUC CV Std: {final_comparison.loc[best_model_cv, 'CV_AUC_Std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des prédictions finales pour la soumission\n",
    "print(\"=== Génération des Prédictions Finales ===\")\n",
    "\n",
    "# Entraîner le meilleur modèle sur l'ensemble complet des données d'entraînement\n",
    "final_model = trained_models[best_model_cv]\n",
    "final_model.fit(X_scaled, y)\n",
    "\n",
    "# Prédictions sur le test set\n",
    "test_predictions_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Créer le fichier de soumission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y': test_predictions_proba\n",
    "})\n",
    "\n",
    "# Sauvegarder le fichier de soumission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"✅ Fichier de soumission créé: submission.csv\")\n",
    "print(f\"   Nombre de prédictions: {len(submission)}\")\n",
    "print(f\"   Modèle utilisé: {best_model_cv}\")\n",
    "print(f\"   Range des prédictions: [{test_predictions_proba.min():.4f}, {test_predictions_proba.max():.4f}]\")\n",
    "print(f\"   Moyenne des prédictions: {test_predictions_proba.mean():.4f}\")\n",
    "\n",
    "# Aperçu du fichier de soumission\n",
    "print(\"\\n=== Aperçu du fichier de soumission ===\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Statistiques des prédictions\n",
    "print(f\"\\n=== Statistiques des Prédictions ===\")\n",
    "print(f\"Prédictions < 0.1: {(test_predictions_proba < 0.1).sum()} ({(test_predictions_proba < 0.1).mean():.2%})\")\n",
    "print(f\"Prédictions 0.1-0.5: {((test_predictions_proba >= 0.1) & (test_predictions_proba < 0.5)).sum()} ({((test_predictions_proba >= 0.1) & (test_predictions_proba < 0.5)).mean():.2%})\")\n",
    "print(f\"Prédictions >= 0.5: {(test_predictions_proba >= 0.5).sum()} ({(test_predictions_proba >= 0.5).mean():.2%})\")\n",
    "\n",
    "# Histogramme des prédictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_predictions_proba, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Probabilité Prédite')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des Probabilités Prédites')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(test_predictions_proba.mean(), color='red', linestyle='--', label=f'Moyenne: {test_predictions_proba.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Classification binaire terminée avec succès!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
